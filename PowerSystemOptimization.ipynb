{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxSxNDFYuq6n",
        "pycharm": {}
      },
      "source": [
        "# Running Pyomo on Google Colab\n",
        "\n",
        "Keywords: installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJHU-Rpkglqq",
        "pycharm": {}
      },
      "source": [
        "This note notebook shows how to install the basic pyomo package on Google Colab, and then demonstrates the subsequent installation and use of various solvers including\n",
        "\n",
        "* GLPK\n",
        "* COIN-OR CBC\n",
        "* COIN-OR Ipopt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJrxAaqcuzFw",
        "pycharm": {}
      },
      "source": [
        "## Basic installation of Pyomo\n",
        "\n",
        "We'll do a quiet installation of pyomo using `pip`.  This needs to be done once at the start of each Colab session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTGBrqQO3vT2",
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "!pip install -q pyomo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pa5VjPfw7Xfm",
        "pycharm": {}
      },
      "source": [
        "The installation of pyomo can be verified by entering a simple model. We'll use the model again in subsequent cells to demonstrate the installation and execution of various solvers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bV-uLVe8yu0",
        "pycharm": {}
      },
      "source": [
        "## COIN-OR CBC installation\n",
        "\n",
        "Keywords: cbc installation\n",
        "\n",
        "[COIN-OR CBC](https://github.com/coin-or/Cbc) is a multi-threaded open-source **C**oin-or **b**ranch and **c**ut mixed-integer linear programming solver written in C++ under the Eclipse Public License (EPL). CBC is generally a good choice for a general purpose MILP solver for medium to large scale problems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFF1UnKYwPOe",
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "!apt-get install -y -qq coinor-cbc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdcuIo9yvA7E",
        "pycharm": {}
      },
      "source": [
        "## COIN-OR Ipopt installation\n",
        "\n",
        "Keywords: Ipopt installation\n",
        "\n",
        "[COIN-OR Ipopt](https://github.com/coin-or/Ipopt) is an open-source **I**nterior **P**oint **Opt**imizer for large-scale nonlinear optimization available under the Eclipse Public License (EPL). It is well-suited to solving nonlinear programming problems without integer or binary constraints."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ipopt installer\n",
        "import sys\n",
        "# If running on Google Colab, install Pyomo and Ipopt via IDAES\n",
        "if \"google.colab\" in sys.modules:\n",
        "    !wget \"https://raw.githubusercontent.com/IDAES/idaes-pse/main/scripts/colab_helper.py\"\n",
        "    import colab_helper\n",
        "    colab_helper.install_idaes()\n",
        "    colab_helper.install_ipopt()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3Ia-jxuF3VY",
        "outputId": "50cd784e-4801-41df-8cd8-8ec81a96f69a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-17 07:43:49--  https://raw.githubusercontent.com/IDAES/idaes-pse/main/scripts/colab_helper.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5459 (5.3K) [text/plain]\n",
            "Saving to: ‘colab_helper.py.5’\n",
            "\n",
            "\rcolab_helper.py.5     0%[                    ]       0  --.-KB/s               \rcolab_helper.py.5   100%[===================>]   5.33K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-10-17 07:43:49 (80.5 MB/s) - ‘colab_helper.py.5’ saved [5459/5459]\n",
            "\n",
            "idaes was found! No need to install.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njG-SkkatYpP",
        "pycharm": {}
      },
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "from agenttraining import *\n",
        "\n",
        "sigma = 0.2\n",
        "epochs = 30\n",
        "batch_size = 256\n",
        "freq = 40\n",
        "fillsize = 256\n",
        "units=32\n",
        "\n",
        "ts = time.time()\n",
        "agent_ddpg = Agent(epochs=epochs,freq=freq,batch_size=batch_size,fillsize=fillsize,sigma=sigma,lr_pi=5e-4,lr_critic=1e-3,tau=0.001,units=units,fill_custom=True)\n",
        "agent_ddpg.train_ddpg()\n",
        "print(f'training time: {time.time()-ts}')\n",
        "\n"
      ],
      "metadata": {
        "id": "_NeZaXsQ3nUC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb899d40-1118-4298-a956-9dc2f4557ad6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# define the model\n",
            "filling buffer....\n",
            "# define the model\n",
            "buffer size: 256\n",
            "training ddpg\n",
            "environment steps: 40, episodes: 0, episode violations: 0, episode rewards: 0.00, test_reward: 0.00, test_violations: 0\n",
            "environment steps: 80, episodes: 0, episode violations: 0, episode rewards: 0.00, test_reward: 0.00, test_violations: 0\n",
            "environment steps: 120, episodes: 0, episode violations: 0, episode rewards: 0.00, test_reward: 0.00, test_violations: 0\n",
            "environment steps: 160, episodes: 0, episode violations: 0, episode rewards: 0.00, test_reward: 0.00, test_violations: 0\n",
            "environment steps: 200, episodes: 0, episode violations: 0, episode rewards: 0.00, test_reward: 0.00, test_violations: 0\n",
            "environment steps: 240, episodes: 0, episode violations: 0, episode rewards: 0.00, test_reward: 0.00, test_violations: 0\n",
            "environment steps: 280, episodes: 0, episode violations: 0, episode rewards: 0.00, test_reward: 0.00, test_violations: 0\n",
            "environment steps: 320, episodes: 0, episode violations: 0, episode rewards: 0.00, test_reward: 0.00, test_violations: 0\n",
            "environment steps: 360, episodes: 0, episode violations: 0, episode rewards: 0.00, test_reward: 0.00, test_violations: 0\n",
            "environment steps: 400, episodes: 1, episode violations: 88, episode rewards: 33.64, test_reward: 0.00, test_violations: 0\n",
            "environment steps: 440, episodes: 1, episode violations: 88, episode rewards: 33.64, test_reward: 0.00, test_violations: 0\n",
            "environment steps: 480, episodes: 1, episode violations: 88, episode rewards: 33.64, test_reward: 0.00, test_violations: 0\n",
            "environment steps: 520, episodes: 1, episode violations: 88, episode rewards: 33.64, test_reward: 0.00, test_violations: 0\n",
            "environment steps: 560, episodes: 1, episode violations: 88, episode rewards: 33.64, test_reward: 0.00, test_violations: 0\n",
            "environment steps: 600, episodes: 1, episode violations: 88, episode rewards: 33.64, test_reward: 0.00, test_violations: 0\n",
            "environment steps: 640, episodes: 1, episode violations: 88, episode rewards: 33.64, test_reward: 0.00, test_violations: 0\n",
            "environment steps: 680, episodes: 1, episode violations: 88, episode rewards: 33.64, test_reward: 0.00, test_violations: 0\n",
            "environment steps: 720, episodes: 1, episode violations: 88, episode rewards: 33.64, test_reward: 0.00, test_violations: 0\n",
            "environment steps: 760, episodes: 1, episode violations: 88, episode rewards: 33.64, test_reward: 0.00, test_violations: 0\n",
            "environment steps: 800, episodes: 2, episode violations: 38, episode rewards: 126.17, test_reward: 0.00, test_violations: 0\n",
            "environment steps: 840, episodes: 2, episode violations: 38, episode rewards: 126.17, test_reward: 0.00, test_violations: 0\n",
            "environment steps: 880, episodes: 2, episode violations: 38, episode rewards: 126.17, test_reward: 0.00, test_violations: 0\n",
            "environment steps: 920, episodes: 2, episode violations: 38, episode rewards: 126.17, test_reward: 0.00, test_violations: 0\n",
            "environment steps: 960, episodes: 2, episode violations: 38, episode rewards: 126.17, test_reward: 0.00, test_violations: 0\n",
            "environment steps: 1000, episodes: 2, episode violations: 38, episode rewards: 126.17, test_reward: 0.00, test_violations: 0\n",
            "environment steps: 1040, episodes: 2, episode violations: 38, episode rewards: 126.17, test_reward: 0.00, test_violations: 0\n",
            "environment steps: 1080, episodes: 2, episode violations: 38, episode rewards: 126.17, test_reward: 0.00, test_violations: 0\n",
            "environment steps: 1120, episodes: 2, episode violations: 38, episode rewards: 126.17, test_reward: 0.00, test_violations: 0\n",
            "environment steps: 1160, episodes: 2, episode violations: 38, episode rewards: 126.17, test_reward: 0.00, test_violations: 0\n",
            "environment steps: 1200, episodes: 3, episode violations: 34, episode rewards: 118.26, test_reward: 0.00, test_violations: 0\n",
            "environment steps: 1240, episodes: 3, episode violations: 34, episode rewards: 118.26, test_reward: 0.00, test_violations: 0\n",
            "environment steps: 1280, episodes: 3, episode violations: 34, episode rewards: 118.26, test_reward: 0.00, test_violations: 0\n",
            "environment steps: 1320, episodes: 3, episode violations: 34, episode rewards: 118.26, test_reward: 0.00, test_violations: 0\n",
            "environment steps: 1360, episodes: 3, episode violations: 34, episode rewards: 118.26, test_reward: 0.00, test_violations: 0\n",
            "environment steps: 1400, episodes: 3, episode violations: 34, episode rewards: 118.26, test_reward: 0.00, test_violations: 0\n",
            "environment steps: 1440, episodes: 3, episode violations: 34, episode rewards: 118.26, test_reward: 0.00, test_violations: 0\n",
            "environment steps: 1480, episodes: 3, episode violations: 34, episode rewards: 118.26, test_reward: 0.00, test_violations: 0\n",
            "environment steps: 1520, episodes: 3, episode violations: 34, episode rewards: 118.26, test_reward: 0.00, test_violations: 0\n",
            "environment steps: 1560, episodes: 3, episode violations: 34, episode rewards: 118.26, test_reward: 0.00, test_violations: 0\n",
            "environment steps: 1600, episodes: 4, episode violations: 27, episode rewards: 147.15, test_reward: 0.00, test_violations: 0\n",
            "environment steps: 1640, episodes: 4, episode violations: 27, episode rewards: 147.15, test_reward: 0.00, test_violations: 0\n",
            "environment steps: 1680, episodes: 4, episode violations: 27, episode rewards: 147.15, test_reward: 0.00, test_violations: 0\n",
            "environment steps: 1720, episodes: 4, episode violations: 27, episode rewards: 147.15, test_reward: 0.00, test_violations: 0\n",
            "environment steps: 1760, episodes: 4, episode violations: 27, episode rewards: 147.15, test_reward: 0.00, test_violations: 0\n",
            "environment steps: 1800, episodes: 4, episode violations: 27, episode rewards: 147.15, test_reward: 0.00, test_violations: 0\n",
            "environment steps: 1840, episodes: 4, episode violations: 27, episode rewards: 147.15, test_reward: 0.00, test_violations: 0\n",
            "environment steps: 1880, episodes: 4, episode violations: 27, episode rewards: 147.15, test_reward: 0.00, test_violations: 0\n",
            "environment steps: 1920, episodes: 4, episode violations: 27, episode rewards: 147.15, test_reward: 0.00, test_violations: 0\n",
            "environment steps: 1960, episodes: 4, episode violations: 27, episode rewards: 147.15, test_reward: 0.00, test_violations: 0\n",
            "# define the model\n",
            "environment steps: 2000, episodes: 5, episode violations: 35, episode rewards: 160.08, test_reward: 238.85, test_violations: 0\n",
            "environment steps: 2040, episodes: 5, episode violations: 35, episode rewards: 160.08, test_reward: 238.85, test_violations: 0\n",
            "environment steps: 2080, episodes: 5, episode violations: 35, episode rewards: 160.08, test_reward: 238.85, test_violations: 0\n",
            "environment steps: 2120, episodes: 5, episode violations: 35, episode rewards: 160.08, test_reward: 238.85, test_violations: 0\n",
            "environment steps: 2160, episodes: 5, episode violations: 35, episode rewards: 160.08, test_reward: 238.85, test_violations: 0\n",
            "environment steps: 2200, episodes: 5, episode violations: 35, episode rewards: 160.08, test_reward: 238.85, test_violations: 0\n",
            "environment steps: 2240, episodes: 5, episode violations: 35, episode rewards: 160.08, test_reward: 238.85, test_violations: 0\n",
            "environment steps: 2280, episodes: 5, episode violations: 35, episode rewards: 160.08, test_reward: 238.85, test_violations: 0\n",
            "environment steps: 2320, episodes: 5, episode violations: 35, episode rewards: 160.08, test_reward: 238.85, test_violations: 0\n",
            "environment steps: 2360, episodes: 5, episode violations: 35, episode rewards: 160.08, test_reward: 238.85, test_violations: 0\n",
            "environment steps: 2400, episodes: 6, episode violations: 29, episode rewards: 175.63, test_reward: 238.85, test_violations: 0\n",
            "environment steps: 2440, episodes: 6, episode violations: 29, episode rewards: 175.63, test_reward: 238.85, test_violations: 0\n",
            "environment steps: 2480, episodes: 6, episode violations: 29, episode rewards: 175.63, test_reward: 238.85, test_violations: 0\n",
            "environment steps: 2520, episodes: 6, episode violations: 29, episode rewards: 175.63, test_reward: 238.85, test_violations: 0\n",
            "environment steps: 2560, episodes: 6, episode violations: 29, episode rewards: 175.63, test_reward: 238.85, test_violations: 0\n",
            "environment steps: 2600, episodes: 6, episode violations: 29, episode rewards: 175.63, test_reward: 238.85, test_violations: 0\n",
            "environment steps: 2640, episodes: 6, episode violations: 29, episode rewards: 175.63, test_reward: 238.85, test_violations: 0\n",
            "environment steps: 2680, episodes: 6, episode violations: 29, episode rewards: 175.63, test_reward: 238.85, test_violations: 0\n",
            "environment steps: 2720, episodes: 6, episode violations: 29, episode rewards: 175.63, test_reward: 238.85, test_violations: 0\n",
            "environment steps: 2760, episodes: 6, episode violations: 29, episode rewards: 175.63, test_reward: 238.85, test_violations: 0\n",
            "environment steps: 2800, episodes: 7, episode violations: 31, episode rewards: 177.36, test_reward: 238.85, test_violations: 0\n",
            "environment steps: 2840, episodes: 7, episode violations: 31, episode rewards: 177.36, test_reward: 238.85, test_violations: 0\n",
            "environment steps: 2880, episodes: 7, episode violations: 31, episode rewards: 177.36, test_reward: 238.85, test_violations: 0\n",
            "environment steps: 2920, episodes: 7, episode violations: 31, episode rewards: 177.36, test_reward: 238.85, test_violations: 0\n",
            "environment steps: 2960, episodes: 7, episode violations: 31, episode rewards: 177.36, test_reward: 238.85, test_violations: 0\n",
            "environment steps: 3000, episodes: 7, episode violations: 31, episode rewards: 177.36, test_reward: 238.85, test_violations: 0\n",
            "environment steps: 3040, episodes: 7, episode violations: 31, episode rewards: 177.36, test_reward: 238.85, test_violations: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.plot(np.array(agent_ddpg.episodic_violations)*100/400,'b--',label='DDPG')\n",
        "plt.title(f'Total violations\\n DDPG:{sum(agent_ddpg.episodic_violations)}')\n",
        "plt.xlabel('episodes')\n",
        "plt.ylabel('% violations/episode')\n",
        "plt.legend(loc='lower right')\n",
        "plt.figure()"
      ],
      "metadata": {
        "id": "ga5OVDLc7jmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.title('Episode Returns')\n",
        "plt.plot(agent_ddpg.episodic_returns,'b--',label='DDPG')\n",
        "plt.xlabel('episodes')\n",
        "plt.ylabel('Returns/episode')\n",
        "plt.legend(loc='lower right')\n"
      ],
      "metadata": {
        "id": "Amzv0SVctBwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.title(f'Test Episode Returns')\n",
        "x = np.array(range(len(agent_ddpg.episodic_violations_test)))*2000/400\n",
        "plt.plot(x,agent_ddpg.episodic_returns_test,'b--',label='DDPG')\n",
        "plt.xlabel('episodes')\n",
        "plt.ylabel('Returns/episode')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aiyYu20rtHiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(x,np.array(agent_ddpg.episodic_violations_test)*100/400,'b--',label='DDPG')\n",
        "plt.title('Test Episode Violations')\n",
        "plt.xlabel('episodes')\n",
        "plt.ylabel('% violations/episode')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HbfnueHBtL4N"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}